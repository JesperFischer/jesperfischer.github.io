---
title: "Joint-Hierarchical-HRD"
author: "Jesper Fischer Ehmsen"
output: html_document
runtime: shiny
date: "2024-06-15"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(
  ordbetareg,
  dplyr,
  ggplot2,
  haven,
  brms,
  tidyr,
  stringr,
  Hmisc,
  shinyApp,
  modelsummary,
  marginaleffects,
  LaplacesDemon,
  extraDistr,
  ggh4x
)


here::set_here()

source(here::here("utility_scripts.R"))

df = read.csv(here::here("raw_hrd.csv"))

df = prep_data(df)
```



# Joint Psychophysical Decision Making Model

Here i present the Joint Psychophysical Decision Making Model (JPDMM), which jointly describes the decision making process in a two-interval forced choice (2IFC) task. The intuition about the model can be seen in the plate-notation below, but contains 3 main sub-modules that all contribute to the overall decision process. Here the platenotation is for a single subject, but can easily be extended to a hierarchical framework.




## Parameters of the model

In order to demonstrate the meaning of the parameters of the model its helpful to first visualize how participants respond to such a task. Here the task represents a 2IFC task where x will represent a sort of stimulus intensity, meaning as x increases you should be more likely to respond that there is a stimulus present. The general idea is then participants are displayed stimuli of varying strength and respond to these of either yes or no. Together with the binary response (r) the reaction time (rt) for this choice is measured and afterwards a confidence rating (conf) is also measured. Here we present both the Intero and Extero condition of the heart rate discrimination (HRD) task.

## Firstly we look at the extero condition and look at the relationship between these 3 variables

```{r, warning = F, message = F, echo = F, fig.width=8,fig.height=6}
#select a subject from the 518 subjects
n_id = 1
  
s_id = unique(df$s)[n_id]

plot = df %>% filter(s == s_id) %>% mutate(Confidence = as.numeric(Confidence),
                                           rt = as.numeric(rt)) %>% rename(Reaction_time = rt, Probability = y) %>% 
  pivot_longer(cols = c("Confidence","Reaction_time","Probability"), names_to = "decision",values_to = "decision_value") %>% 
  ggplot()+
  geom_point(aes(x = x, y = decision_value, col = Modality))+
  facet_wrap(~decision, scales = "free", ncol = 1)+theme_classic()
  

plot
```

## Overlaying some kind of curve fitted by ggplot2's geom_smooth method using a loess method we get the following:


```{r, warning = F, message = F, echo = F, fig.width=8,fig.height=6}
#select a subject from the 518 subjects
n_id = 1
  
s_id = unique(df$s)[n_id]

plot = df %>% filter(s == s_id) %>% mutate(Confidence = as.numeric(Confidence),
                                           rt = as.numeric(rt)) %>% rename(Reaction_time = rt, Probability = y) %>% 
  pivot_longer(cols = c("Confidence","Reaction_time","Probability"), names_to = "decision",values_to = "decision_value") %>% 
  ggplot(aes(x = x, y = decision_value, col = Modality))+
  geom_point()+
  facet_wrap(~decision, scales = "free", ncol = 1)+theme_classic()+ geom_smooth()
  

plot
```

## Intution of the model 

The pattern of responses is quite easily visable, and can be argued to be intuitive on several levels. Firstly the middle section of the binary responses follow the usual pattern of increasing "1" / "yes" / "faster" responses as the stimulus intensity increases. The reaction times follow a pattern where participants are responding fast (low RTs) when very low and or very high stimuli is presented with slower reaction times (high RTs) close the transisition point of the probability graph. Lastly confidence ratings follow the oppisite pattern to the reaction time with lowest confidence close to the transition between "1" and "0" responses, with highest ratings in the extreme stimulus values. This pattern and intuition can easily be mathematically explained, if we think generatively about the process of the participant. We assume that the participant has a function that maps stimuli values to probabilities of responding "1", this function we call a psychometric function and takes the stimuli values and maps them to probabilities $p = \Psi(x | \Theta)$ here $\Theta$ represents parameters of the psychometric function. Common psychometric function include the cumulative normal and logistic distributions, also known in the generalized linear modeling framework as probit and logit link functions when doing logistic regression. What these functions have in common is that they have 2-4 parameters. Here we consider the 3 parameter cumulative normal distribution as the psychometric function which has the following mathematical formulation:

$$
p_t = \Psi(x_t | \alpha, \beta, \lambda) = \lambda + (1 - 2 * \lambda) + \left(0.5+0.5 \cdot erf\left(\frac{x_t-\alpha}{\beta * \sqrt2}\right)\right)
$$

here the parameters $\alpha$ $\beta$ $\lambda$ all govern the shape of the psychometric function with $\alpha$ being the location of the function (i.e the x-value) when y = 0.5, $\beta$ determines the steepness of the function and $\lambda$ controls both extreme ends of the function such that if $\lambda$ = 0 the function extends to y = 1 and y = 0 in the extremes, but if $\lambda$ = 0.1 it extends to y = 1 - 0.1 = 0.9 and y = 0 + 0.1 = 0.1.

The output of such psychometric function is thus the probability of responding "1", which are then thought to be inputs to the Bernoulli distribution (again through the generative process) that then has an output either 0 and 1's. The Bernoulli distribution is a 1 parameter distribution i.e. the probability parameter, the mean of this distribution is therefore also this probability parameter, but its variance is given by $Var(Bern(p)) = p \cdot (1-p)$. This means that the uncertainty in the distribution it-self follows a second order polynomial that peaks at 0.5 and tapers off when this probability parameter either increases or decreases. Note this is because of the fact that the probability parameter is bounded between [0 ; 1] and when transformed through the variance of the Bernoulli distribution you will have the most uncertainty at p = 0.5 and the least at p = 0 & p = 1 where the variance is 0.


We use the variance of the Bernoulli distribution to guide the functional relationship between the psychometric function (that outputs the probabilities) and the functions that govern the reaction times and Confidence ratings. We believe that the generative process of reaction times on these binary choices follow the same relationship as the variance of the underlying Bernoulli distribution that govern the binary responses. This means that the mean of the reaction times could be a linear mapping of an intercept and a slope on this variance i.e.

$$
\mu_{t_{rt}} \sim Int_{rt} + \beta_{rt} * Var(Bern(p_t))
$$
where $Int_{rt}$ represents the intercept and $\beta_{RT}$ represents the degree to which the uncertainty from the psychometric function influences the reaction times. In order to stochastically model the reaction times with this formulation a probability density function is needed to account for the noise observed. Due to the non negative nature of reactions times and the physical constraints of information processing (i.e. a delay from the time the stimulus is presented to which it reaches the brain of the agent), a sensible choice of this probability density function would be the shifted log normal distribution. This introduces two more variables, a non decision time ($\tau$) and a standard deviation ($\sigma_{rt}$) for the log normal distribution. This formulation of the reactions times therefore follow the relationship described below, where the crucial link, linking the psychometric function and the reaction times being the Bernoulli variance.

$$
RT_t \sim LN(Int_{rt} + \beta_{RT} * Var(Bern(p_t), \sigma_{rt}) + \tau
$$
where $LN$ represent the lognormal distribution. 


The same argumentation can be done for the mean of the confidence ratings (where we would expect the slope i.e. $\beta_{Conf}$ to be negative). In order to accurately describe the distribution of confidence ratings a bounded response likelihood function is needed as the confidence ratings are between 0 and 1 but includes 0 and 1. Normally a bounded response variable between 0 and 1 would entail a beta-distribution, but the edge cases of 0 and 1 makes this troublesome. Quite a few likelihood functions exsist to try and describe this kind of behavior, but the most promising and parsimonious choice here is argued to be the ordered beta likelihood from [Kubinec](https://www.cambridge.org/core/journals/political-analysis/article/abs/ordered-beta-regression-a-parsimonious-wellfitting-model-for-continuous-data-with-lower-and-upper-bounds/89F4141DA16D4FC217809B5EB45EEE83). This model basically jointly and parsimoniously models the "degenerate" 0 and 1 responses as well as the values between. The likelihood function does this by introducing two cut-points initialized by a dirichlet prior in the following manner:
$$
\begin{equation}
\begin{cases}
\alpha = 1 - g(X'\beta - k_1) \\
\delta = [g(X'\beta - k_1) - g(X'\beta - k_2)] \cdot \text{Beta}(g(X'\beta), \phi) \\
\gamma = g(X'\beta - k_2)
\end{cases}
\tag{1}
\end{equation}
$$
where $\alpha$ represents the probability of a 0 response, $\delta$ represents the probability of a ]0 ; 1[ response and $\gamma$ represents the probability of a 1 response. $k_1$ and $k_2$ represents the two cut-points estimated by the model which controls the amount of "degenerateness" of the responses. $X$ represents the design matrix of independent variables and $\beta$ represents a vector of parameter coefficients for these independent variables. Beta(.) represents the beta distribution parameterized as mean and precision with $\phi$ being the precision parameter. Lastly, g(.) represents the "link" function which here is the inverse logit transformation mapping the real line to values in [0 ; 1]. 

This means that for the simplest model the matrix multiplication of $X'\beta$ can be rewritten as:

$$
X'\beta = \mu_{Confidence_t} = Int_{Conf} + \beta_{Conf} * Var(Bern(p_t)
$$

## Understanding the parameters of the model

Many parameters have been introduced and the meaning of each one might be hard to understand in isolation. Below is therefore a Shiny app that allows for a user to slide the parameters of the model and see how it changes the underlying functions: Note that here a few parameters do not do anything which are the non decision time for the reaction times i.e. rt_ndt = $\tau$ as well as the two standard deviations of the beta and logNormal distribtion i.e. conf_prec = $\sigma_{Conf}$ and rt_sd = $\sigma_{rt}$. This is because we are only looking at how the mean predictions change and not generating data points (yet).


```{r, warning = F, message = F, echo = F}

source(here::here("utility_scripts.R"))

df = read.csv(here::here("raw_hrd.csv")) %>% filter(participant_id == "sub-0019" & Modality == "Extero")

dd = prep_data(df)

shinyApp(
# Define UI
ui <- navbarPage(
  "Parameters",
    tabPanel("JPDM", 
           sidebarLayout(
             sidebarPanel(
               sliderInput("beta", "slope", min = -3, max = 3, value = 0.1, step = 0.1),
               sliderInput("lapse", "lapse", min = -10, max = 0, value = -4, step = 0.01),
               sliderInput("alpha", "alpha", min = -50, max = 50, value = 0, step = 0.1),
               sliderInput("conf_int", "conf_int", min = -3, max = 3, value = -1, step = 0.1),
               sliderInput("conf_beta", "conf_beta", min = -20, max = 5, value = -5, step = 0.1),
               sliderInput("conf_shift", "conf_shift", min = -10, max = 10, value = 0, step = 0.1),
               sliderInput("conf_cutzero", "conf_cutzero", min = -10, max = 10, value = 0, step = 0.1),
               sliderInput("conf_cutone", "conf_cutone", min = -10, max = 10, value = 0, step = 0.1),
               
               sliderInput("rt_int", "rt_int", min = -3, max = 3, value = -0.5, step = 0.01),
               sliderInput("rt_beta", "rt_beta", min = -3, max = 3, value = 1.5, step = 0.1),
               sliderInput("rt_ndt", "rt_ndt", min = -10, max = 0, value = -3, step = 0.1),
               sliderInput("rt_shift", "rt_shift", min = -10, max = 10, value = 0, step = 0.1)
               
             ),
             mainPanel(
               plotOutput("results", height = "900px")
             )
           )
  )
),

# Define server
server <- function(input, output) {
  
  
    
  output$results <- renderPlot({

    
    
      jpdm = function(x,beta,lapse,alpha,
              conf_int,conf_beta,conf_prec,conf_shift,
              rt_int,rt_beta,rt_sd,rt_ndt,rt_shift, 
              id = 1){
  
probab = function(x,lapse,beta,alpha){
  return((brms::inv_logit_scaled(lapse) / 2) + (1 - 2 *  (brms::inv_logit_scaled(lapse) / 2)) * (0.5+0.5 * pracma::erf((x-alpha) / (exp(beta) * sqrt(2)))))
  
}
    prob = probab(x,lapse,beta,alpha)
    
    conf = brms::inv_logit_scaled(conf_int + conf_beta * (probab(x-conf_shift,lapse,beta,alpha) * (1-probab(x-conf_shift,lapse,beta,alpha))))
    
    rt = exp(rt_int + rt_beta * (probab(x-rt_shift,lapse,beta,alpha) * (1-probab(x-rt_shift,lapse,beta,alpha))))
    
    return(data.frame(prob = prob,rt = rt, conf  = conf,x = x, id = id))
}
  # input = data.frame(beta = 1, lapse = 0.05, alpha = 5, conf_int = 1, conf_beta = -5, conf_prec = 10,conf_shift = -2,
  #                    rt_int = 2, rt_beta = 2, rt_sd = 1, rt_ndt = 1,rt_shift = 2, id = 1)  
  # 
    data <- jpdm(x = seq(-50,50,0.1), beta = input$beta, lapse = input$lapse, alpha = input$alpha,
                           conf_int = input$conf_int, conf_beta = input$conf_beta, conf_prec = input$conf_prec, input$conf_shift,
                           rt_int = input$rt_int, rt_beta = input$rt_beta, rt_sd = input$rt_sd, rt_ndt = input$rt_ndt, input$rt_shift, id = 1)
    

    #overlay data?
  
    dd = dd %>% mutate(conf = as.numeric(Confidence) / 100,
                                             rt = as.numeric(rt), prob = y) %>% 
    pivot_longer(cols = c("prob","rt","conf"), names_to = "decision",values_to = "decision_value")

    
    data = data %>% pivot_longer(cols = c("prob","rt","conf"), names_to = "decision",values_to = "decision_value")
    
    plot1 = dd %>% 
      ggplot()+
      geom_point(data = dd, aes(x = x, y = decision_value))+
      geom_line(data = data,aes(x = x, y = decision_value))+
      facet_wrap(~decision, scales = "free",ncol = 1)+theme_classic()+
      theme(text = element_text(size = 24))
        


    plot1 = plot1+
    facetted_pos_scales(
      y = list(
        decision == "prob" ~ scale_y_continuous(limits = c(0, 1), breaks = c(0,0.2,0.4,0.6,0.8,1.0)),
        decision == "rt" ~ scale_y_continuous(limits = c(0, 8), breaks = c(0,2,4,6,8)),
        decision == "conf" ~ scale_y_continuous(limits = c(0, 1), breaks = c(0,0.2,0.4,0.6,0.8,1.0))
      )
    )
    
    
    plot1
  })
  
},
options = list(height = 1200, width = 1200)

)
```


## Inverting the model based on real data.




# Hierarchical modeling!
  